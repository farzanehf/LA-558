scale_fill_viridis_c(option = "C")+
theme_minimal()+
theme(axis.text.x = element_blank(), axis.text.y = element_blank(),
axis.ticks = element_blank(),
panel.background = element_blank(),
legend.text = element_text(size=18,face="bold"),
legend.title=element_text(size=18,face="bold"))+
labs(fill="Prediction")
jura_sf_loc = st_as_sf(jura_locs,coords=c("Xlog","Yloc"))
jura_k_pred = krige(Zn ~ 1, jura_sf_loc, jura_grid, jura_vm_sph)
jura_sf_loc = st_as_sf(jura_locs,coords=c("Xlog","Yloc"))
jura_k_pred = krige(Zn ~ 1, jura_sf, jura_grid, jura_vm_sph)
jura_k_pred
library(ggplot2)
ggplot() + geom_stars(data = jura_k_pred, aes(fill = var1.pred, x = x, y = y)) +
scale_fill_viridis_c(option = "C")+
theme_minimal()+
theme(axis.text.x = element_blank(), axis.text.y = element_blank(),
axis.ticks = element_blank(),
panel.background = element_blank(),
legend.text = element_text(size=18,face="bold"),
legend.title=element_text(size=18,face="bold"))+
labs(fill="Prediction")
jura_sf_loc = st_as_sf(jura_locs,coords=c("Xlog","Yloc"))
jura_k_pred = krige(Zn ~ 1, jura_sf, jura_v, jura_vm_sph)
jura_k_pred = krige(Zn ~ 1, jura_sf, jura_locs, jura_vm_sph)
jura_sf_loc = st_as_sf(jura_locs,coords=c("Xlog","Yloc"))
jura_k_pred = krige(Zn ~ 1, jura_sf, jura_grid, jura_vm_sph)
jura_k_pred
library(ggplot2)
ggplot() + geom_stars(data = jura_k_pred, aes(fill = var1.pred, x = x, y = y)) +
scale_fill_viridis_c(option = "C")+
theme_minimal()+
theme(axis.text.x = element_blank(), axis.text.y = element_blank(),
axis.ticks = element_blank(),
panel.background = element_blank(),
legend.text = element_text(size=18,face="bold"),
legend.title=element_text(size=18,face="bold"))+
labs(fill="Prediction")
jura_cv = krige.cv(Zn~1, jura_sf, jura_vm_sph)
jura_cv
sqrt(mean(jura_cv$residual^2))
jura_cv = krige.cv(Zn~1, jura_sf, jura_vm_sph)
MSEP = jura_cv$residual^2
rMSEP = sqrt(mean(jura_cv$residual^2))
rMSEP = sqrt(mean(jura_cv$residual^2))
jura_cv = krige.cv(Zn~1, jura_sf, jura_vm_sph)
MSEP = jura_cv$residual^2
MSEP
rMSEP = sqrt(mean(jura_cv$residual^2))
rMSEP
MSEP
rMSEP
hist(jura_sf$Zn, col=Zn)
hist(jura_sf$Zn)
hist(jura_sf$Zn)
hist(log(jura_sf$Zn))
# Cressie-Hawkins estimator
jura.cressie_log = variogram(log(Zn~1), jura_sf, cressie=T)
# Cressie-Hawkins estimator
jura.cressie_log = variogram((log(Zn)~1), jura_sf, cressie=T)
jura.cressie_log
plot(jura.cressie_log)
# Spherical with nugget
jura.v_log = variogram((log(Zn)~1), jura_sf)
jura_vm_sph_log = fit.variogram(jura.v_log, vgm(698, 'Sph', 0.27, 171))
jura_vm_sph_log
plot(jura.v_log, jura_vm_sph)
jura_ssv = variogram(log(Zn)~1, jura_sf)
# can also specify log(rain) or I(1/rain), using I() to protect the 1/rain
jura_ssph = fit.variogram(jura_ssv, vgm(698,'Sph', 0.27,171))
# TG predictions
jura_tgk = krigeTg(Zn~1, jura_sf, jura_grid, jura_ssph, lambda=0 )
jura_ssv = variogram(log(Zn)~1, jura_sf)
# can also specify log(rain) or I(1/rain), using I() to protect the 1/rain
jura_ssph = fit.variogram(jura_ssv, vgm(698,'Sph', 0.27,171))
# TG predictions
jura_tgk = krigeTg(Zn~1, jura_sf, jura_grid, jura_ssph, lambda=0 )
jura_tgk
plot(jura_tgk)
ggplot() + geom_stars(data = jura_tgk, aes(fill = var1.pred, x = x, y = y)) +
scale_fill_viridis_c(option = "C")+
theme_minimal()+
theme(axis.text.x = element_blank(), axis.text.y = element_blank(),
axis.ticks = element_blank(),
panel.background = element_blank(),
legend.text = element_text(size=18,face="bold"),
legend.title=element_text(size=18,face="bold"))+
labs(fill="Prediction")
jura_ssv = variogram(log(Zn)~1, jura_sf)
# can also specify log(rain) or I(1/rain), using I() to protect the 1/rain
jura_ssph = fit.variogram(jura_ssv, vgm(698,'Sph', 0.27,171))
# TG predictions
jura_tgk = krigeTg(Zn~1, jura_sf, jura_grid, jura_ssph, lambda=0 )
jura_tgk
ggplot() + geom_stars(data = jura_tgk, aes(fill = exp(var1.pred), x = x, y = y)) +
scale_fill_viridis_c(option = "C")+
theme_minimal()+
theme(axis.text.x = element_blank(), axis.text.y = element_blank(),
axis.ticks = element_blank(),
panel.background = element_blank(),
legend.text = element_text(size=18,face="bold"),
legend.title=element_text(size=18,face="bold"))+
labs(fill="Prediction")
plot(jura_k_pred$var1.pred,exp(jura_tgk$var1.pred))
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## ----install-packages----------------------------------------------------
install.packages(c("tidycensus", "tidyverse", "geofacet", "ggridges"))
install.packages(c("tidycensus", "tidyverse", "geofacet", "ggridges"))
knitr::opts_chunk$set(echo = TRUE)
## ----api-key-------------------------------------------------------------
library(tidycensus)
##
census_api_key("YOUR KEY GOES HERE", install = TRUE)
## ----api-key-------------------------------------------------------------
library(tidycensus)
## ----install-packages----------------------------------------------------
install.packages(c("tidycensus", "tidyverse", "geofacet", "ggridges"))
install.packages(c("tidycensus", "tidyverse", "geofacet", "ggridges"))
## ----api-key-------------------------------------------------------------
library(tidycensus)
.libPaths()
<html>
<html>
## ----api-key-------------------------------------------------------------
## library(tidycensus)
##
census_api_key("b2981571606707ca6960874ecd1cfa3d8631b0f2", install = TRUE,  overwrite=TRUE)
#ex6a
install.packages(c("tidycensus", "tidyverse", "geofacet", "ggridges"))
## ----api-key-------------------------------------------------------------
library(tidycensus)
##
census_api_key("b2981571606707ca6960874ecd1cfa3d8631b0f2", install = TRUE,  overwrite=TRUE)
# Retrieve the data on population density for all states
pop_density <- get_acs(geography = "state",
variables = "B01003_001E,B01003_001M,B01003_001EA",
year = 2019)
# Clean up the data
pop_density <- pop_density %>%
mutate(population = estimate_B01003_001E,
std_error = margin_B01003_001M,
moe = margin_B01003_001EA) %>%
select(-estimate_B01003_001E, -margin_B01003_001M, -margin_B01003_001EA) %>%
mutate(pop_density = population / aland * 2589988.11) # Calculate population density (people per square mile)
library(tidyverse)
## ----api-key-------------------------------------------------------------
census_api_key("b2981571606707ca6960874ecd1cfa3d8631b0f2", install = TRUE,  overwrite=TRUE)
# Retrieve the data on population density for all states
pop_density <- get_acs(geography = "state",
variables = "B01003_001E,B01003_001M,B01003_001EA",
year = 2019)
# Clean up the data
pop_density <- pop_density %>%
mutate(population = estimate_B01003_001E,
std_error = margin_B01003_001M,
moe = margin_B01003_001EA) %>%
select(-estimate_B01003_001E, -margin_B01003_001M, -margin_B01003_001EA) %>%
mutate(pop_density = population / aland * 2589988.11) # Calculate population density (people per square mile)
## ----decennial-------------------------------------------------------------------------
pop20 <- get_decennial(
geography = "state",
variables = "P1_001N",
year = 2020
)
## ----view-decennial--------------------------------------------------------------------
pop20
pop20$variable
pop20$NAME
pop20$value
# Clean up the data
pop_density <- pop_density %>%
mutate(population = estimate_B01003_001E,
std_error = margin_B01003_001M,
moe = margin_B01003_001EA) %>%
select(-estimate_B01003_001E, -margin_B01003_001M, -margin_B01003_001EA) %>%
mutate(pop_density = population / aland * 2589988.11) # Calculate population density (people per square mile)
pop_density
# Clean up the data
pop_density <- pop_density %>%
mutate(population = estimate_B01003_001E,
std_error = margin_B01003_001M,
moe = margin_B01003_001EA) %>%
select(-estimate_B01003_001E, -margin_B01003_001M, -margin_B01003_001EA) %>%
mutate(pop_density = population / aland * 2589988.11) # Calculate population density (people per square mile)
# Retrieve the data on population density for all states
pop_density <- get_acs(geography = "state",
variables = "B01003_001E,B01003_001M,B01003_001EA",
year = 2019)
## ----api-key-------------------------------------------------------------
census_api_key("b2981571606707ca6960874ecd1cfa3d8631b0f2", install = TRUE,  overwrite=TRUE)
table_p2 <- get_decennial(
geography = "state",
table = "P2",
year = 2020
)
library(tidycensus)
library(tidyverse)
library(tidytext)
## ----api-key-------------------------------------------------------------
census_api_key("b2981571606707ca6960874ecd1cfa3d8631b0f2", install = TRUE,  overwrite=TRUE)
# Get population data by state from TidyCensus
pop_data <- get_acs(geography = "state",
variables = c("POP", "POP_LAST", "B01001_001"),
year = 2020,
survey = "acs5")
View(pop_data)
# Calculate percent population change from 2019 to 2020
pop_data$pop_change <- (pop_data$POP - pop_data$POP_LAST) / pop_data$POP_LAST * 100
# Calculate percent population change from 2019 to 2020
pop_data$pop_change <- (pop_data$POP - pop_data$POP_LAST) / pop_data$POP_LAST * 100
# Get population data by state from TidyCensus
pop_data <- get_acs(geography = "state",
variables = c("POP", "POP_LAST", "B01001_001"),
year = 2020,
survey = "acs5")
# Calculate percent population change from 2019 to 2020
pop_data$pop_change <- (pop_data$POP - pop_data$POP_LAST) / pop_data$POP_LAST * 100
# Get 2019 population data for comparison
pop_data_last <- get_acs(geography = "state",
variables = c("POP"),
year = 2019,
survey = "acs5")
# Merge population data
pop_data <- left_join(pop_data, pop_data_last, by = "NAME")
# Get 2019 population data for comparison
pop_data_last <- get_acs(geography = "state",
variables = c("POP"),
year = 2019,
survey = "acs5")
# Get 2019 population data for comparison
pop_data_last <- get_acs(geography = "state",
variables = c("POP"),
year = 2019,
survey = "acs5")
# Get population data by state from TidyCensus
pop_data_last <- get_acs(geography = "state",
variables = c("POP", "B01001_001"),
year = 2020,
survey = "acs5")
# Get population data by state from TidyCensus
pop_data <- get_acs(geography = "state",
variables = c("POP", "B01001_001"),
year = 2020,
survey = "acs5")
# Get 2019 population data for comparison
pop_data_last <- get_acs(geography = "state",
variables = c("POP"),
year = 2019,
survey = "acs5")
library(tidyverse)
library(tidycensus)
library(sf)
library(tmap)
# Get population data by state from TidyCensus
pop_data <- get_acs(geography = "state",
variables = c("POP", "B01001_001"),
year = 2020,
survey = "acs5")
# Get 2019 population data for comparison
pop_data_last <- get_acs(geography = "state",
variables = c("POP"),
year = 2019,
survey = "acs5")
c2020 <- load_variables(2020, "pl")
View(c2020)
#2010 version - only 301 entries
c2010 <- load_variables(2020, "pl")
aian_2020 <- get_decennial(
geography = "state",
variables = "P1_005N",
year = 2020,
sumfile = "pl"
)
View(aian_2020)
elementary <- get_decennial(geography = "school district (elementary)",
year = 2020)
elementary <- get_decennial(geography = "school district (elementary)",
state = IA
year = 2020)
elementary <- get_decennial(geography = "school district (elementary)",
state = "IA"
year = 2020)
elementary <- get_decennial(geography = "school district (elementary)",
state = "IA",
year = 2020)
elementary <- get_decennial(geography = "school district (elementary)",
state = "IA",
year = 2020)
install.packages(c("tidycensus", "tidyverse", "geofacet", "ggridges"))
# Load necessary packages
library(tidycensus)
library(tidyverse)
library(tidytext)
library(sf)
library(tmap)
install.packages(c("tidycensus", "tidyverse", "geofacet", "ggridges"))
library(tidycensus)
library(tidyverse)
library(tidytext)
library(sf)
library(tmap)
## ----api-key-------------------------------------------------------------
census_api_key("b2981571606707ca6960874ecd1cfa3d8631b0f2", install = TRUE,  overwrite=TRUE)
aian_2020 <- get_decennial(
geography = "state",
variables = "P1_005N",
year = 2020,
sumfile = "pl"
)
elementary <- get_decennial(geography = "school district (elementary)",
state = "IA",
year = 2020)
elementary <- get_decennial(geography = "urban area",
state = "IA",
year = 2020)
windmills_counties$total_t_cap[is.na(windmills_counties$total_t_cap)] <- rnorm(sum(is.na(windmills_counties$total_t_cap)), mean = fit$estimate[1], sd = fit$estimate[2])
#install.packages("tidyverse")
library(tidyverse)
#still have to load this separate from tidyverse
#install.packages("readxl")
library(readxl)
### Set source location to the same as this file
setwd("C:/Spring 2023/CRP 558/LA-558/Assignment7")
#read in excel file
windmills <- read_excel("uswtdb_v4_3_20220114.xlsx")
#Summarize by state
windmills_states <- windmills %>%
group_by(t_state) %>%
summarise(total_t_cap = sum(t_cap), count_of_windmills = n())
#Summarize by state and county
windmills_counties <- windmills %>%
group_by(t_state, t_county) %>%
summarise(total_t_cap = sum(t_cap), count_of_windmills = n())
#write CSVs for the files
write_csv(windmills_states, "windmills_states.csv")
### Set source location to the same as this file
setwd("C:/Spring 2023/CRP 558/LA-558/Assignment7")
#read in excel file
windmills <- read_excel("uswtdb_v4_3_20220114.xlsx")
#Summarize by state
windmills_states <- windmills %>%
group_by(t_state) %>%
summarise(total_t_cap = sum(t_cap), count_of_windmills = n())
#Summarize by state and county
windmills_counties <- windmills %>%
group_by(t_state, t_county) %>%
summarise(total_t_cap = sum(t_cap), count_of_windmills = n())
#write CSVs for the files
write_csv(windmills_states, "windmills_states.csv")
write_csv(windmills_counties, "windmills_counties.csv")
# Load the library for MLE
library(maxLik)
# Create a function to calculate the log-likelihood of the normal distribution
loglik <- function(par, x){
mu <- par[1]
sigma <- par[2]
sum(log(dnorm(x[!is.na(x)], mean = mu, sd = sigma)))
}
# Subset the data with non-missing values of income
mydata <- windmills_counties$total_t_cap[!is.na(windmills_counties$total_t_cap)]
# Define the log-likelihood function for the normal distribution
loglik_norm <- function(par){
loglik(par, x = mydata)
}
# Use MLE to estimate the mean and variance of the normal distribution
fit <- maxLik(loglik_norm, start = c(mean(mydata), sd(mydata)))
# Use the estimated mean and variance to impute missing values
windmills_counties$total_t_cap[is.na(windmills_counties$total_t_cap)] <- rnorm(sum(is.na(windmills_counties$total_t_cap)), mean = fit$estimate[1], sd = fit$estimate[2])
windmills_counties_positive <- windmills_counties                       # Duplicate vector
windmills_counties_positive[windmills_counties_positive < 0] <- 0       # Set negative values to 0
windmills_counties_positive                                             # Print updated vector
write_csv(windmills_counties_positive, "windmills_counties_noNA.csv")
#install.packages("tidyverse")
library(tidyverse)
#still have to load this separate from tidyverse
#install.packages("readxl")
library(readxl)
### Set source location to the same as this file
setwd("C:/Spring 2023/CRP 558/LA-558/Assignment7")
#read in excel file
windmills <- read_excel("uswtdb_v4_3_20220114.xlsx")
#Summarize by state
windmills_states <- windmills %>%
group_by(t_state) %>%
summarise(total_t_cap = sum(t_cap), count_of_windmills = n())
#Summarize by state and county
windmills_counties <- windmills %>%
group_by(t_state, t_county) %>%
summarise(total_t_cap = sum(t_cap), count_of_windmills = n())
#write CSVs for the files
write_csv(windmills_states, "windmills_states.csv")
write_csv(windmills_counties, "windmills_counties.csv")
##########
### For windmills_counties
# Load the library for MLE
library(maxLik)
# Create a function to calculate the log-likelihood of the normal distribution
loglik <- function(par, x){
mu <- par[1]
sigma <- par[2]
sum(log(dnorm(x[!is.na(x)], mean = mu, sd = sigma)))
}
# Subset the data with non-missing values of income
mydata <- windmills_counties$total_t_cap[!is.na(windmills_counties$total_t_cap)]
# Define the log-likelihood function for the normal distribution
loglik_norm <- function(par){
loglik(par, x = mydata)
}
# Use MLE to estimate the mean and variance of the normal distribution
fit <- maxLik(loglik_norm, start = c(mean(mydata), sd(mydata)))
# Use the estimated mean and variance to impute missing values
windmills_counties$total_t_cap[is.na(windmills_counties$total_t_cap)] <- rnorm(sum(is.na(windmills_counties$total_t_cap)), mean = fit$estimate[1], sd = fit$estimate[2])
windmills_counties_positive <- windmills_counties                       # Duplicate vector
windmills_counties_positive[windmills_counties_positive < 0] <- 0       # Set negative values to 0
windmills_counties_positive                                             # Print updated vector
write_csv(windmills_counties_positive, "windmills_counties_noNA_Positive.csv")
########
##########
### For windmills_states
# Load the library for MLE
library(maxLik)
# Create a function to calculate the log-likelihood of the normal distribution
loglik <- function(par, x){
mu <- par[1]
sigma <- par[2]
sum(log(dnorm(x[!is.na(x)], mean = mu, sd = sigma)))
}
# Subset the data with non-missing values of income
mydata2 <- windmills_states$total_t_cap[!is.na(windmills_states$total_t_cap)]
# Define the log-likelihood function for the normal distribution
loglik_norm <- function(par){
loglik(par, x = mydata2)
}
# Use MLE to estimate the mean and variance of the normal distribution
fit <- maxLik(loglik_norm, start = c(mean(mydata2), sd(mydata2)))
# Use the estimated mean and variance to impute missing values
windmills_states$total_t_cap[is.na(windmills_states$total_t_cap)] <- rnorm(sum(is.na(windmills_states$total_t_cap)), mean = fit$estimate[1], sd = fit$estimate[2])
windmills_states_positive <- windmills_states                       # Duplicate vector
windmills_states_positive[windmills_states_positive < 0] <- 0       # Set negative values to 0
windmills_states_positive
write_csv(windmills_states_positive, "windmills_states_noNA_Positive.csv")
#install.packages("tidyverse")
library(tidyverse)
#still have to load this separate from tidyverse
#install.packages("readxl")
library(readxl)
### Set source location to the same as this file
setwd("C:/Spring 2023/CRP 558/LA-558/Final Project")
#read in excel file
windmills <- read_excel("uswtdb_v4_3_20220114.xlsx")
#Summarize by state
windmills_states <- windmills %>%
group_by(t_state) %>%
summarise(total_t_cap = sum(t_cap), count_of_windmills = n())
#Summarize by state and county
windmills_counties <- windmills %>%
group_by(t_state, t_county) %>%
summarise(total_t_cap = sum(t_cap), count_of_windmills = n())
#write CSVs for the files
write_csv(windmills_states, "windmills_states.csv")
write_csv(windmills_counties, "windmills_counties.csv")
##########
### For windmills_counties
# Load the library for MLE
library(maxLik)
# Create a function to calculate the log-likelihood of the normal distribution
loglik <- function(par, x){
mu <- par[1]
sigma <- par[2]
sum(log(dnorm(x[!is.na(x)], mean = mu, sd = sigma)))
}
# Subset the data with non-missing values of income
mydata <- windmills_counties$total_t_cap[!is.na(windmills_counties$total_t_cap)]
# Define the log-likelihood function for the normal distribution
loglik_norm <- function(par){
loglik(par, x = mydata)
}
# Use MLE to estimate the mean and variance of the normal distribution
fit <- maxLik(loglik_norm, start = c(mean(mydata), sd(mydata)))
# Use the estimated mean and variance to impute missing values
windmills_counties$total_t_cap[is.na(windmills_counties$total_t_cap)] <- rnorm(sum(is.na(windmills_counties$total_t_cap)), mean = fit$estimate[1], sd = fit$estimate[2])
windmills_counties_positive <- windmills_counties                       # Duplicate vector
windmills_counties_positive[windmills_counties_positive < 0] <- 0       # Set negative values to 0
windmills_counties_positive                                             # Print updated vector
write_csv(windmills_counties_positive, "windmills_counties_noNA_Positive.csv")
########
##########
### For windmills_states
# Load the library for MLE
library(maxLik)
# Create a function to calculate the log-likelihood of the normal distribution
loglik <- function(par, x){
mu <- par[1]
sigma <- par[2]
sum(log(dnorm(x[!is.na(x)], mean = mu, sd = sigma)))
}
# Subset the data with non-missing values of income
mydata2 <- windmills_states$total_t_cap[!is.na(windmills_states$total_t_cap)]
# Define the log-likelihood function for the normal distribution
loglik_norm <- function(par){
loglik(par, x = mydata2)
}
# Use MLE to estimate the mean and variance of the normal distribution
fit <- maxLik(loglik_norm, start = c(mean(mydata2), sd(mydata2)))
# Use the estimated mean and variance to impute missing values
windmills_states$total_t_cap[is.na(windmills_states$total_t_cap)] <- rnorm(sum(is.na(windmills_states$total_t_cap)), mean = fit$estimate[1], sd = fit$estimate[2])
windmills_states_positive <- windmills_states                       # Duplicate vector
windmills_states_positive[windmills_states_positive < 0] <- 0       # Set negative values to 0
windmills_states_positive
write_csv(windmills_states_positive, "windmills_states_noNA_Positive.csv")
