woodlice=read.csv('woodlice.csv', as.is=T)
head(woodlice)
library(knitr)
#### 1-a
woodlice%>%ggplot(aes(x=x,y=y))+
geom_point(aes(size=count))+theme_bw()+
xlim(0,5.7)+
ylim(0,5.2)+
geom_point(x=3.4641016,y=0.0,color="Blue",size=3)+
geom_point(x=5.6291651,y=2.5980762,color="Green",size=6)+
geom_point(x=1.2990381,y=0.8660254,color="red",size=0)+
geom_text(x=5.6291651,y=2.5980762,label="Best Positive Clustering",color="Green")+
geom_text(x=3.4641016,y=0.0,label="Another Positive Clustering",color="Blue")+
geom_text(x=1.2990381,y=0.8660254,label="Negative Clustering",color="red")
library(dplyr)
woodlice.sf = st_as_sf(woodlice,coords=c("x","y"))
woodlice_nb <- dnearneigh(woodlice.sf, 0, 1)
table(sapply(woodlice_nb,length))
#Results says that we have 6 locations with 3 neighbors, 12 locations with 4 neighbors and 19 locations with 6 neighbors.
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
row.names(London_shp)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(spdep)  # areal dependence functions using sp objects
library(rgdal)  # for readOGR() to read ArcGIS shape files
library(spData) # sp data files, includes Auckland shape file
library(stars)
woodlice=read.csv('woodlice.csv', as.is=T)
head(woodlice)
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(spdep)  # areal dependence functions using sp objects
library(rgdal)  # for readOGR() to read ArcGIS shape files
library(spData) # sp data files, includes Auckland shape file
library(stars)
woodlice=read.csv('woodlice.csv', as.is=T)
head(woodlice)
library(knitr)
#### 1-a
woodlice%>%ggplot(aes(x=x,y=y))+
geom_point(aes(size=count))+theme_bw()+
xlim(0,5.7)+
ylim(0,5.2)+
geom_point(x=3.4641016,y=0.0,color="Blue",size=3)+
geom_point(x=5.6291651,y=2.5980762,color="Green",size=6)+
geom_point(x=1.2990381,y=0.8660254,color="red",size=0)+
geom_text(x=5.6291651,y=2.5980762,label="Best Positive Clustering",color="Green")+
geom_text(x=3.4641016,y=0.0,label="Another Positive Clustering",color="Blue")+
geom_text(x=1.2990381,y=0.8660254,label="Negative Clustering",color="red")
#### 1-a
woodlice%>%ggplot(aes(x=x,y=y))+
geom_point(aes(size=count))+theme_bw()+
xlim(0,5.7)+
ylim(0,5.2)+
geom_point(x=3.4641016,y=0.0,color="Blue",size=3)+
geom_point(x=5.6291651,y=2.5980762,color="Green",size=6)+
geom_point(x=1.2990381,y=0.8660254,color="red",size=0)+
geom_text(x=5.6291651,y=2.5980762,label="Best Positive Clustering",color="Green")+
geom_text(x=3.4641016,y=0.0,label="Another Positive Clustering",color="Blue")+
geom_text(x=1.2990381,y=0.8660254,label="Negative Clustering",color="red")
install.packages("ggplot2")
library(ggplot2)
woodlice.sf = st_as_sf(woodlice,coords=c("x","y"))
woodlice_nb <- dnearneigh(woodlice.sf, 0, 1)
table(sapply(woodlice_nb,length))
#Results says that we have 6 locations with 3 neighbors, 12 locations with 4 neighbors and 19 locations with 6 neighbors.
sd_wb = nb2listw(woodlice_nb, style='B')
woodlace_moran = moran.mc(woodlice.sf$count, sd_wb, 9999)
woodlace_moran
woodlace_geary = geary.mc(woodlice.sf$count, sd_wb, 9999)
woodlace_geary
sd_w = nb2listw(woodlice_nb, style='W')
woodlace_moran_w = moran.mc(woodlice.sf$count, sd_w, 9999)
woodlace_moran_w
woodlace_geary_w = geary.mc(woodlice.sf$count, sd_w, 9999)
woodlace_geary_w
# For tranforming the count data:
woodlice.sf$sqrt<-sqrt(woodlice.sf$count)
woodlice.sf$sqrt
# moran test:
woodlace_moran_sqrt<-moran.mc(woodlice.sf$sqrt, sd_w,9999)
woodlace_moran_sqrt
woodlace_geary_sqrt<-geary.mc(woodlice.sf$sqrt, sd_w, 9999)
woodlace_geary_sqrt
woodlice.sf$status = ifelse(woodlice.sf$count<=0,"Absence","Presence")
woodlice.sf$status = factor(woodlice.sf$status)
woodlice.sf$status
## join count monte carlo
joincount_mc = joincount.mc(woodlice.sf$status, sd_wb, 9999)
joincount_mc
woodlice.sf$status = if(woodlice.sf$count>0) {
"Presence"
} else {
"Absence"
}
woodlice.sf$status = factor(woodlice.sf$status)
woodlice.sf$status
## join count monte carlo
joincount_mc = joincount.mc(woodlice.sf$status, sd_wb, 9999)
setwd("C:/Spring 2022/Stat 576/HW #3/London")
London_shp = readShapePoly("london/LDNSuicides.shp")
setwd("C:/Spring 2022/Stat 576/HW #3/London")
London_shp = readShapePoly("london/LDNSuicides.shp")
London_shp = readShapePoly("LDNSuicides.shp")
London_shp = readShapePoly("london/LDNSuicides.shp")
London_csv<-read.csv("london/london.csv")
setwd("C:/Spring 2022/Stat 576/HW #3/London/London")
London_shp = readShapePoly("london/LDNSuicides.shp")
London_shp = readShapePoly("LDNSuicides.shp")
setwd("C:/Spring 2022/Stat 576/HW #3/London/London")
London_shp = readShapePoly("LDNSuicides.shp")
London_csv<-read.csv("london.csv")
# C:\Spring 2022\Stat 576\HW #3\London\London
setwd("C:/Spring 2022/Stat 576/HW #3/London/London")
London_shp = readShapePoly("LDNSuicides.shp")
London_shp = st_read("LDNSuicides.shp")
London_shp = readShapePoly("LDNSuicides.shp")
# C:\Spring 2022\Stat 576\HW #3\London\London
library(rgdal)
setwd("C:/Spring 2022/Stat 576/HW #3/London/London")
London_shp = readShapePoly("LDNSuicides.shp")
#### 1-a
woodlice%>%ggplot(aes(x=x,y=y))+
geom_point(aes(size=count))+theme_bw()+
xlim(0,5.7)+
ylim(0,5.2)+
geom_point(x=3.4641016,y=0.0,color="Blue",size=3)+
geom_point(x=5.6291651,y=2.5980762,color="Green",size=6)+
geom_point(x=1.2990381,y=0.8660254,color="red",size=0)+
geom_text(x=5.6291651,y=2.5980762,label="Best Positive Clustering",color="Green")+
geom_text(x=3.4641016,y=0.0,label="Another Positive Clustering",color="Blue")+
geom_text(x=1.2990381,y=0.8660254,label="Negative Clustering",color="red")
#### 1-b
woodlice.sf = st_as_sf(woodlice,coords=c("x","y"))
woodlice_nb <- dnearneigh(woodlice.sf, 0, 1)
table(sapply(woodlice_nb,length))
sd_wb = nb2listw(woodlice_nb, style='B')
woodlace_moran = moran.mc(woodlice.sf$count, sd_wb, 9999)
woodlace_moran
woodlace_geary = geary.mc(woodlice.sf$count, sd_wb, 9999)
woodlace_geary
library(rgdal)
setwd("C:/Spring 2022/Stat 576/HW #3/London/London")
London_shp = readShapePoly("LDNSuicides.shp")
London_csv<-read.csv("london.csv")
library("gpclib")
install.packages("gpclib")
# C:\Spring 2022\Stat 576\HW #3\London\London
library("gpclib")
library("maptools")
install.packages("maptools")
library("maptools")
setwd("C:/Spring 2022/Stat 576/HW #3/London/London")
London_shp = readShapePoly("LDNSuicides.shp")
London_csv<-read.csv("london.csv")
London_shp = readShapePoly("LDNSuicides.shp")
# C:\Spring 2022\Stat 576\HW #3\London\London
library("gpclib")
library("maptools")
setwd("C:/Spring 2022/Stat 576/HW #3/London/London")
London_shp = readShapePoly("LDNSuicides.shp")
London_csv<-read.csv("london.csv")
plot(London_shp)
London_shp$ID1=row.names(London_shp)
London_csv$ID2=row.names(London_csv)
shp.csv=merge(London_shp, London_csv, by.x = "ID1", by.y = "ID2")
spplot(shp.csv, zcol="logSMR",col=NA)
# I got in this dataset, shapefile and csv file row numbers are not the same so the resullt is wrong. Because shapefile starts from 0, and csv file starts from 1,  So for having the same row numbers for both, I added 1 to shapefile row number.
# #############
# for adding 1 to row number, first I chnage it to numeric then add 1.
London_shp$ID1 = as.numeric(London_shp$ID1)
London_shp$ID1 = London_shp$ID1 + 1
shp.csv=merge(London_shp, London_csv, by.x = "ID1", by.y = "ID2")
spplot(shp.csv, zcol="logSMR",col=NA)
shp.csv$response = London.data$logSMR
shp.csv$response = London_csv$logSMR
str(shp.csv)
shp.csv$value = London_csv$logSMR
shp.csv$dif = shp.csv$value-shp.csv$eblup
shp.csv$value = London_csv$logSMR
shp.csv$dif = shp.csv$value-shp.csv$eblup
#### 2-d
str(shp.csv)
shp.csv$value = London_csv$logSMR
shp.csv$dif = shp.csv$value-shp.csv$eblup
shp.csv
View
View(shp.csv)
View(shp.csv$value)
View(shp.csv$eblup)
View(shp.csv$eblup)
logSMRsmooth$eblup
View(logSMRsmooth$eblup)
#### 2-c
logSMRsmooth <- eblupFH(logSMR~1, varSMR, data=London_csv)
library(sf)
library(spdep)  # areal dependence functions using sp objects
library(rgdal)  # for readOGR() to read ArcGIS shape files
library(spData) # sp data files, includes Auckland shape file
library(stars)
woodlice=read.csv('woodlice.csv', as.is=T)
head(woodlice)
#library(knitr)
library(rmdformats)
library(ggplot2)
library(dplyr)
library(sp)
library(maptools)
library(sae)
library(dplyr)
#### 1-a
woodlice%>%ggplot(aes(x=x,y=y))+
geom_point(aes(size=count))+theme_bw()+
xlim(0,5.7)+
ylim(0,5.2)+
geom_point(x=3.4641016,y=0.0,color="Blue",size=3)+
geom_point(x=5.6291651,y=2.5980762,color="Green",size=6)+
geom_point(x=1.2990381,y=0.8660254,color="red",size=0)+
geom_text(x=5.6291651,y=2.5980762,label="Best Positive Clustering",color="Green")+
geom_text(x=3.4641016,y=0.0,label="Another Positive Clustering",color="Blue")+
geom_text(x=1.2990381,y=0.8660254,label="Negative Clustering",color="red")
# The best points for the positive are the points that have similar size and are large. For the negative point it is the best to be small that surronded between large points.
#### 1-b
woodlice.sf = st_as_sf(woodlice,coords=c("x","y"))
woodlice_nb <- dnearneigh(woodlice.sf, 0, 1)
table(sapply(woodlice_nb,length))
#Results says that we have 6 locations with 3 neighbors, 12 locations with 4 neighbors and 19 locations with 6 neighbors.
#### 1-c
sd_wb = nb2listw(woodlice_nb, style='B')
woodlace_moran = moran.mc(woodlice.sf$count, sd_wb, 9999)
woodlace_moran
woodlace_geary = geary.mc(woodlice.sf$count, sd_wb, 9999)
woodlace_geary
#### 1-d
# I used Monte Carlo simulation because we only have 37 samples, and it is very small.
# So, for having more sample size and using shuffling I did Monte Carlo simulation.
#### 1-e
sd_w = nb2listw(woodlice_nb, style='W')
woodlace_moran_w = moran.mc(woodlice.sf$count, sd_w, 9999)
woodlace_moran_w
woodlace_geary_w = geary.mc(woodlice.sf$count, sd_w, 9999)
woodlace_geary_w
#### 1-f
# For tranforming the count data:
woodlice.sf$sqrt<-sqrt(woodlice.sf$count)
woodlice.sf$sqrt
# moran test:
woodlace_moran_sqrt<-moran.mc(woodlice.sf$sqrt, sd_w,9999)
woodlace_moran_sqrt
woodlace_geary_sqrt<-geary.mc(woodlice.sf$sqrt, sd_w, 9999)
woodlace_geary_sqrt
#### 1-g
woodlice.sf$status = ifelse(woodlice.sf$count<=0,"Absence","Presence")
woodlice.sf$status = factor(woodlice.sf$status)
woodlice.sf$status
# join count monte carlo
joincount_mc = joincount.mc(woodlice.sf$status, sd_wb, 9999)
joincount_mc
#### 1-h
#### 2-a
library("gpclib")
library("maptools")
setwd("C:/Spring 2022/Stat 576/HW #3/London/London")
London_shp = readShapePoly("LDNSuicides.shp")
London_csv<-read.csv("london.csv")
plot(London_shp)
London_shp$ID1=row.names(London_shp)
London_csv$ID2=row.names(London_csv)
shp.csv=merge(London_shp, London_csv, by.x = "ID1", by.y = "ID2")
spplot(shp.csv, zcol="logSMR",col=NA)
# I got in this dataset, shapefile and csv file row numbers are not the same so the resullt is wrong.
# Because shapefile starts from 0, and csv file starts from 1,  So for having the same row numbers for both, I added 1 to shapefile row number.
# for adding 1 to row number, first I chnage it to numeric then add 1.
London_shp$ID1 = as.numeric(London_shp$ID1)
London_shp$ID1 = London_shp$ID1 + 1
shp.csv=merge(London_shp, London_csv, by.x = "ID1", by.y = "ID2")
spplot(shp.csv, zcol="logSMR",col=NA)
#### 2-b
london_nb = poly2nb(London_shp)
table(sapply(london_nb,length))
#### 2-c
logSMRsmooth <- eblupFH(logSMR~1, varSMR, data=London_csv)
shp.csv$eblup <- logSMRsmooth$eblup
spplot(shp.csv, zcol="eblup",col=NA)
#### 2-d
str(shp.csv)
shp.csv$value = London_csv$logSMR
shp.csv$dif = shp.csv$value-shp.csv$eblup
spplot(shp.csv, zcol="dif",col=NA)
# plot
shp.csv@data%>%ggplot()+
geom_point(aes(x=eblup,y=value),size=3)+
xlim(-0.5,0.7)+
ylim(-0.5,0.7)+
geom_abline(intercept = 0,slope = 1)
setwd("/Users/kkhan/Dropbox/ISU/STAT406/Lectures/Week-5-PP/code")
# point2.r  -  estimate intensity
# using spatstat functions for spatial point patterns
# swamp tree data is the example
swamp=read.csv('swamp.csv',header=T)
# has 4 columns: x, y, live/dead, and species code
cypress=swamp[swamp$live == 1 & swamp$sp=='TD',]
# keep cypress (Taxodium distichum) locations
library(spatstat)
# for point pattern analysis, need boundaries of study area
# for this is a rectangle with x = (0,50), y = (0,200)
# a point pattern is a ppp object
cypress_ppp=as.ppp(cypress[,c('x','y')], W=c(0,50, 0, 200))
cypress_ppp=as.ppp(cypress[,c('y','x')], W=c(0,200, 0, 50))
plot(density(cypress_ppp, 1.5))
# density computes the kernel smoothed estimate of intensity
#   first arg is the ppp object, second is the sd of the kernel
#     sd is also called bandwidth
#   smaller sd = coarser pattern, larger = smoother pattern
plot(density(cypress_ppp, 15))
#  compare the two plots
# how to choose sd?  various ways
#  bw.diggle() returns the most commonly used estimator of bandwidth
bw.diggle(cypress_ppp)
# my experience is that this often undersmooths, in part because of a limited
#   range of possible bandwidths
# can ask to search over larger range
bw.diggle(cypress_ppp, hmax=10)
# can save the result and plot it to see the entire curve
temp= bw.diggle(cypress_ppp)
plot(temp)
# but often have to deal with curiosities, like huge mse at small BW
plot(temp, xlim=c(0.5, 3) )
# one way is to not plot values below a specified minimum
temp= bw.diggle(cypress.ppp, hmax=10)
plot(temp)
plot(temp, xlim=c(0.5,6))
# the last plot is good enough to give you a sense that MSE is pretty flat
#   for a large range of potential values
# --------------
# point process likelihood method of choosing BW:
bw.ppl(cypress_ppp)
# and again, can save and plot to see the curve
temp= bw.ppl(cypress_ppp)
plot(temp)
# again hugely bad BW when very small
temp=bw.ppl(cypress_ppp, srange=c(1, 40))
# specify range to consider as srange: vector of minimum and maximum
plot(temp)
plot(temp, xlim=c(5, 40))
# again restrict plotted range to see the interesting parts
temp=bw.ppl(cypress_ppp, srange=c(5,20))
plot(temp)
# and can get more detail by shrinking the "interesting" range
# can use automatically by providing the function name as sd:
plot(density(cypress_ppp, bw.diggle))
------------------------------------------
# modeling intensity as a function of geographic coordinates
cypress_ipp=ppm(cypress_ppp, ~x+y, correction='iso')
# 1st arg is point process, 2nd is trend model
#   correction = specifies the edge correction: iso = isotropic=Ripley
#  can add interaction models if desired, e.g. Strauss
cypress_ipp
# a summary of the trend infromation, including the estimated parameters
#   and tests of = 0
summary(cypress_ipp)
# and a lot more details.
# to test whether any linear trend, compare to a model with constant mean
cypress_hpp= ppm(cypress_ppp, ~1,correction="iso")
anova(cypress_hpp, cypress_ipp, test='Chi')
# LR test (also called analysis of deviance test)
# valid for Poisson processes, not for pseudolikelihood (i.e. interactions)
AIC(cypress_ipp)
AIC(cypress_hpp)
# or use model selection ideas and compare AIC statistics
# it is possible to model intensity as a function other covariates
#   for the swamp tree dataset, the ecologically relevant covariate
#   is water depth
# to fit such a model, water depth is needed at all locations, not just
#  event locations.  Not trivial!
# can fit such models if you have additional observations of the covariate(s)
#   at some non-event locations
# see me if you need to do this.
# using spatstat functions for spatial point patterns
# swamp tree data is the example
swamp=read.csv('swamp.csv',header=T)
# has 4 columns: x, y, live/dead, and species code
cypress=swamp[swamp$live == 1 & swamp$sp=='TD',]
# keep cypress (Taxodium distichum) locations
library(spatstat)
# for point pattern analysis, need boundaries of study area
# for this is a rectangle with x = (0,50), y = (0,200)
# a point pattern is a ppp object
cypress_ppp=as.ppp(cypress[,c('x','y')], W=c(0,50, 0, 200))
cypress_ppp=as.ppp(cypress[,c('y','x')], W=c(0,200, 0, 50))
# can use various formats, see ?as.ppp for details, easiest is:
# 1st arg is a data frame with x and y variables
#   deliberately only keep x and y coordinates.
# 2nd arg is 4 element vector with xmin, xmax, ymin, ymax
#   that specifies the observation window.  Obs window does NOT
#   have to be rectangular.  If an arbitrary polygon, specify as
#   corners of the polygon.
plot(cypress_ppp)
# Nearest-neighbor distance function
G =Gest(cypress_ppp)
# can also specify distances, but default is often very reasonable
# get 3 values for estimated G function
#  for different ways to correct for edge effects
# and theoretical G fn
plot(G)
# Monte-Carlo tests:
Genv=envelope(cypress_ppp, Gest, nsim=999, nrank = 25)
# 1st arg is data set
# 2nd is function used to compute function
# nsim = # simulations
# nrank = rank of desired quantile
#   25'th of (nsim+1) is 0.025 quantile, also get 1-0.025 = 0.975
plot(Genv)
# plots envelope and observed G curve
# the four lines are: OBServed, THEOretical, and
#   HIgh and LOw boundaries of envelope
# point-event distance function
F=Fest(cypress_ppp)
plot(F)
Fenv=envelope(cypress_ppp, Fest, nsim=999, nrank = 25)
plot(Fenv)
# Ripley's K function
K = Kest(cypress_ppp)
plot(K)
Kenv=envelope(cypress_ppp, Kest, nsim=999, nrank = 25)
plot(Kenv)
# and the L() transformation
L=Lest(cypress_ppp)
plot(L)
# I prefer L() - r so CRS gives a flat line and plot
#   highlights differences from CSR
L2 = L
# first column is r, rest are different ests of L
L2[,2:5]= as.data.frame(L2)[,2:5] - L$r
# as.data.frame() is needed to force R to do the calculation
# easiest way to get the simulation envelope for L() - r:
Lenv=envelope(cypress_ppp, Lest, nsim=999, nrank=25)
plot(Lenv)
# second through 5th cols are L() functions
names(Lenv)
# double check that: r is distance, rest are 4 diff ests. of L(r)
Lenv2=Lenv
Lenv2[,2:5]=as.data.frame(Lenv2)[,2:5] - Lenv$r
plot(Lenv2)
plot(Lenv2, legendargs=list(cex=0.8))
# reduce text size in legend so it doesn't overplot
# want to find the distance where Lhat crosses outside of envelope
min(Lenv2$r[Lenv2$obs > Lenv2$hi])
# and finally the pair correlation function
g=pcf(cypress_ppp)
plot(g)
genv=envelope(cypress_ppp, pcf, nsim=999, nrank=25)
plot(genv)
plot(genv, ylim=c(0,5))   # to eliminate (from the plot) the huge spike near 0
# also want to find the distance where ghat crosses outside of envelope
min(genv$r[genv$obs > genv$hi], na.rm=T)
# genv not evaluated at r=0, so value is na.  na.rm=T says to ignore that
# you should be able to explain why gobs crosses outside its envelope
#  at a smaller distance than does Kobs.
# my preference is for ln g,
#  can get that same way as we transformed L()'s
genv2=genv
genv2[,2:5] =log(as.data.frame(genv2)[,2:5])
plot(genv2, ylim=c(-5,5))
# basic plot
plot(genv2, ylim=c(-5,5), legendpos='topright', legendargs=list(cex=0.7))
# control legend position and size of text (cex=)
# or directly specify the log transform in the envelope command
genv3=envelope(cypress_ppp, pcf, nsim=999, nrank=25,
transform=expression(log(.)) )
# . represents the function being computed, here the output from pcf()
plot(genv3)
# using spatstat functions for spatial point patterns
# swamp tree data is the example
swamp=read.csv('swamp.csv',header=T)
cypress=swamp[swamp$live == 1 & swamp$sp=='TD',]
library(spatstat)
# a point pattern is a ppp object
cypress_ppp=as.ppp(cypress[,c('x','y')], W=c(0,50, 0, 200))
cypress_ppp=as.ppp(cypress[,c('y','x')], W=c(0,200, 0, 50))
plot(cypress_ppp)
G =Gest(cypress_ppp)
# can also specify distances, but default is often very reasonable
# get 3 values for estimated G function
#  for different ways to correct for edge effects
# and theoretical G fn
plot(G)
Genv=envelope(cypress_ppp, Gest, nsim=999, nrank = 25)
plot(Genv)
# point-event distance function
F=Fest(cypress_ppp)
plot(F)
Fenv=envelope(cypress_ppp, Fest, nsim=999, nrank = 25)
plot(Fenv)
# Ripley's K function
K = Kest(cypress_ppp)
plot(K)
Kenv=envelope(cypress_ppp, Kest, nsim=999, nrank = 25)
plot(Kenv)
# and the L() transformation
L=Lest(cypress_ppp)
plot(L)
L2 = L
L2[,2:5]= as.data.frame(L2)[,2:5] - L$r
Lenv=envelope(cypress_ppp, Lest, nsim=999, nrank=25)
plot(Lenv)
plot(Lenv)
plot(Lenv)
